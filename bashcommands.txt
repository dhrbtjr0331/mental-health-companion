To run the Tinyllama model:
./bin/llama-cli -m ../../llama-models/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf -p "Give me three coping strategies for anxiety."

To run the llama-server:
./bin/llama-server -m ../../llama-models/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf --port 8080